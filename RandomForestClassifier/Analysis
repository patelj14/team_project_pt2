Expanding on the first team project attempt (linear regression and logistic regression fitting with no hyperparameters changes or cross validation), the random forest classifier was tried with hyperparameter tuning conducted with a grid search, cross-validation and a pipeline including linear (standard scaling) or non linear transformations (robust scaler and power transformer).

Furthermore, some categorical features of input data were found to be in need of one-hot encoding as they were not ordinal categories as their values suggested nor should some of them have zero values for category (according to dataset source description), this was for first and third datasets. For the second dataset, some variables were found to have extreme outliers which were removed. 

Hyperparameters varied included number of estimators (25,50,100,200) and max features considered when looking for best split (sqrt of n features or 1.0). Models were ranked by classification accuracy. 

The all combined dataset continued to perform poorly with a best test accuracy of 0.54 with 25 estimators and 1 max feature. This was worse than the logistic regression score of 0.57. 

For the first_third dataset, the best test accuracy was 0.83 and train accuracy ws 0.83. The corresponding logistic regression test accuracy was 0.73 and training accuracy was 0.80. Therefore, the random forest showed a significant increase in test accuracy and also avoided the overfitting that occured with the logistic regression (with the latter having much higher training accuracy than test accuracy). The most important feature was found to be ST_slope, which dominated the importance metric and was much larger than all others (0.28 vs <0.09 for all others), with chest_pain_type second and restingecg third most important. The logistic regression also had ST_slope as most important feature (0.23) much higher than others (<0.01), with exercise_angina seconda and chest_pain_type third. Hence, ST_slope seems to most important factor by far, with chest_pain_type also playing some role. 

For the second dataset, the best test accuracy was 0.71 and train accuracy was 0.70. For the previous logistic regression, the test accuracy was 0.72 and training accuracy was 0.73. This time, both methods have roughly similar accuracies and lack of overfitting. The most important features were found to be ap_hi (0.31), age (0.24), weight (0.21) and height (0.18). As can be seen, many features have importance of similar magnitude and it is not dominated by a single feature as in the first_third dataset. For the previous corresponding logistic regression, the most important features were ap_hi (0.12), cholesterol (0.03) and age (0.02). Here, there is again a bigger difference between the most important feature and others and cholesterol is ranked second, while in the random forest it was ranked much lower at sixth. 

Overall, it can be seen that the random forest classifier's performance greatly depends on the dataset being fit. For the first_third dataset, it offered significant advantage and produced the highest accuracy of all datasets and methods (0.83). For the second dataset it produced roughly the same accuracy as logistic regression and for the combined all dataset (with only two input features, the lowest quality dataset), it performed worse. The random forest models found the same most important feature for the first_third and second datasets, while order of the remaining features and their feature importance was different. The fact that cross-validation and hyperparameter tuning was performed provides more confidence in the random forest results. On the other hand, random forests are known to struggle with multiple category features (specifically when one-hot encoded). 

This study also highlights the importance of looking at different data sources (which might have incompatable features) to get a broad understanding. From two different models we were able to identify ST_slope, ap_hi, age and weight as important features. 

 


